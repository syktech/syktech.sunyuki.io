---
layout:     post
title:      "浅谈Java内存模型"
subtitle:   "虽然JSR-133在2004年就给出了Java内存模型和线程规范，但是我在这里依然还是要写这个内容，其目的为了加深自己在这块的理解，另外就是让公司团队也强化一下在多线程上面的造化。"
date:       2016-08-20 16:17:00
author:     "ryan"
header-img: "img/post-bg-06.jpg"
---


# 1. 前序
我们还是稍微增加一些门槛，怎么使用java多线程的类和关键字，以及每个关键字的含义，这里就不多做讲解，必定这个属于java基础，本章的内容都是界定大家都已经熟悉java并发编程基础展开的讲解。废话不多说直接上代码：

```java
public class VolatileExample {
	 int x = 0;
	 int y = 0;
	 int a = 0;
	 int b = 0;
	
	
	public void go() throws Exception {
	    Thread one = new Thread(new Runnable() {
	        public void run() {
	            a = 1;
	            x = b;
	        }
	    });

	    Thread other = new Thread(new Runnable() {
	        public void run() {
	            b = 1;
	            y = a;
	        }
	    });
	    one.start();other.start();
	    one.join();other.join();
    	if (x == 0 && y == 0) {
    		System.out.println("(" + x + "," + y + ")");
    	} else if (x == 1 && y == 1) {
    		System.out.println("(" + x + "," + y + ")");
    	}
	}
}
```

执行go方法，在one thread和other thread跑完后(该机器处理器型号Intel(R) Xeon(R) CPU E5-2430，为什么要说明处理，后面的内容会说，我们会得到(0,1), (1,0), (0,0)这三个结果，这里我们就产生几个疑问了：

1.为什么没有(1,1)的结果？

2.按程序的执行顺序应该是(0,1)才会，为什么又多出来(1,0)，(0,0);

带着这些疑问我们开始讲解我们今天的内容





# 2. Java线程调度

线程调度是为线程分配处理器的使用权限的过程，主要的调用方式有两种：一种是协同式线程调度和抢占式线程调度。

### 2.1 协同式线程调度

线程执行时间由自己来控制，它执行完后才会通知系统，让系统去调用下一个线程，因为不会在该线程中途将CPU的使用权切换的其他线程上面，而是等它执行完才通知系统切换另外一个，所以就不会存在多线程的同步问题，因为它本来就是同步的。该方式唯一的好处就是实现简单，无同步问题，但是坏处也非常突出，如果一个线程出了问题一直占用不通知系统，那么后续的线程会一直阻塞；以前windows 3.x系统使用的调度模式，导致系统极度不稳定，一个程序进程出了问题，会导致整个系统crash down。

### 2.2 抢占式线程调度

每个线程有系统来控制，就不会出现协同式线程调度的一个出问题导致所有阻塞的问题， Java虚拟机就是用该模式来调度线程，当然缺点就是同步问题了，这个问题就是我们今天要说的JMM需要解决的。抢占式线程调度从名字来看已经知道，分配给每个线程的使用CPU的时间是不定的，谁抢到谁先用，而且也不是一直用到线程结束，中间还可以被其他线程抢走，当然Java Thread提供的线程的优先级来设置，setPriority方法一共有三个值MIN, NORMAL, MAX，默认是NORMAL，当然MAX获取CPU的资源会相对更多了。

### 2.3 分析前序DEMO

回到之前前序的DEMO，有时候出现(1,0)，有时候出现(0,1)，我们就知道为什么了，我们肉眼看见one比other先调用start，由于Java虚拟机是抢占式线程调度，所以如果在那一瞬间one先开始，other后开始，但是由于系统CPU资源可能正在执行其他程序指令时，在等待系统调度时都有同等的机会，other thread是有可能抢在one之前开始执行的，所以会有(1,0)的情况，反之one先抢到那么就会是(0,1)的结果；抢占式线程调度，我刚才也说当一个线程执行的指令比较多的时候，也不会等你所有指令执行完了才切换到其他线程，可能中途系统就切换到其他线程，这样交替执行，所以也可能出现(1,1)，但是为什么我们演示没有出现呢，我判断的是因为我们目前的处理都是多核而且高频率，所以对于短的指令很可能在每次切换前就执行完了，下面代码可以印证(1,1)出现的可能性，而且也印证每次交替之前执行指令之多；特别是当CPU很忙的时候交替频次越高。

```java
public class VolatileExampleForeach {
	public void go() throws Exception {
	    Thread one = new Thread(new Runnable() {
	        public void run() {
	            for(int i = 0; i < 10000; i++) {
	            	System.out.println(Thread.currentThread().getName());
	            }
	        }
	    });
	    
	    Thread other = new Thread(new Runnable() {
	        public void run() {
	        	for(int i = 0; i < 10000; i++) {
	            	System.out.println(Thread.currentThread().getName());
	            }
	        }
	    });
	    one.start();other.start();
	    one.join();other.join();
	}
}
```

打印结果出现是交替的，但是交替期很长，到这里我们理解为什么会出现(1,0),(0,1)，以及为们该出现的(1,1)没有出现，还剩下(0,0)这个奇葩的值，这个值出现是有重排序或者可见性导致的，需要下面的知识点来解释了；





# 3. 重排序

重排序是指不同类型的CPU为了让其性能达到最大的一种优化措施，处理器里面有很多级流水线，越现代的处理级数越高，每级流水线都并行负责处理从寄存器传递过来的指令，有些指令处理时间长，有些指令处理时间短，为了让每级流水线都能饱和工作，处理器加入了乱序处理模块，也就是将后续指令会提前到一些时间长的指令之前执行掉，这就产生了重排序(Intel从Pentium Pro这块处理开始引入乱序处理模块)。



### 3.1 as-if-serial

就算重排序，所有类型的处理器也会as-if-serial的语义来重排序，as-if-serial的语义是如果在单线程中前后执行的指令之间有依赖关系，是不会重排序的，当然这个强调是单线程，在多线程是之间存在依赖关系肯定是感知不了，所以我这里强调在单线程里面，再换句话说有第一条指令依赖第二条，如果到序那执行结果必将不同，所以也可以这样定义as-if-serial的语义是不影响结果的重排序，为了更加说明这点，我举了如下例子：

```java
1 int a = 1;
2 int b = 2
3 int c = a + b;
```

1和2之间是允许被重排序的，但是1和3，2和3是不允许的，因为他们之间有结果的依赖关系；



### 3.2 排序类型

有4种类型的重排序：load-store, load-load, store-load, store-store，不同类型的操作系统对他们支持度是不同的，支持度越高，那么这个处理的性能最大化就越强，下面表格描述：

|            | load-load | load-store | store-store | store-load |
| ---------- | --------- | ---------- | ----------- | ---------- |
| x86/x86-64 | 不支持       | 不支持        | 不支持         | 支持         |
| IA64       | 支持        | 支持         | 支持          | 支持         |
| PowerPC    | 支持        | 支持         | 支持          | 支持         |

x86这项应该是我们最熟悉的体系架构，这个架构下的CPU占有大致99%以上市场份额(intel和amd厂商)，因为大部分的操作系统都是x86体系下的，IA64是intel的安腾架构体系，这个CPU也很少见，但是他的64位处理性能远远高于x86-64(也一般用在服务器上)，有了IA64的架构体系，为什么x86-64还是这么普及，其原因是x86体系比ia64早出来，占领了市场，x86-64是兼容了之前的x86的所以就这么普及了(这个体系架构最早是AMD公司提出来的)。另外一个PowerPC，最早这个处理器是IBM推出来的，最早用在了现在很出名的APPLE PC上面，但是后面APPLE PC转战到了intel的x86体系下面直到今天，另外PowerPC的架构体系，被google收购的摩托罗拉之前的大部分芯片也是用了这个架构体系，现在PowerPC体系下的CPU也只有在服务器上面使用了。说了真么多，我们还没有分别解释这4种排序的意思：

#### load-load:

读1指令能和读2以及读2之后的读指令进行排序；

#### load-store:

读1指令能和写2以及写2之后的写指令进行排序；

#### store-store:

写1指令能和写2以及写2之后的写指令进行排序；

#### store-load:

写1指令能和读2以及读2之后的读指令进行排序；



### 3.3 分析前序DEMO

我们还剩下一个(0,0)的结果没有解释，有了上面的知识，我想这个结果也就有些理所当然了，看下面的分析；

one thread:

> set a=1, get b, set x=b;

other thread:

> set b=1, get a, set y=a;

因为store-load这种类型，x86体系是支持的，xeon处理器属于x86体系的，所以有可能出现下面的执行顺序；

| 线程           | 时段1      | 时段2      | 时段3     | 时段4        |
| ------------ | -------- | -------- | ------- | ---------- |
| one-thread   | get b(0) |          | set a=1 | set x=b(0) |
| other-thread |          | get a(0) | set b=1 | set y=a(0) |

上面这是将get b和get a排到当前线程其他指令之前执行，导致获取到的都是0，最后set x和y都是使用的是0值来设置，所以可以解释这种现象了，但是除了指令的重排序会导致外，还有一种可能也会导致，那就是内存模型中的可见性，后面的内容我们将说说为们可见性也会导致(0,0)的出现。





# 4. 可见性

先来看一张图大致了解一下CPU是如何操纵内存数据的

 ![20160828_jmm_thread_visibility](https://ryanwli.github.io/img/2016/20160828_jmm_thread_visibility.png)

现在的CPU都是多核，以及都有自己高速缓存器，从存储设备的速度来排序大致是这样，CPU Cache > Memory > SSD > HDD，最早期的CPU是直接从Main Memory操纵数据，太慢了，后来设计了处理的高速缓存来代替直接从Main Memory操纵数据，从而让处理性能有指数倍的进步。但是这种速度提升了但是带来了不同处理线程之间对于共享数据的延迟性，如上图所示我可能在第一个CPU的Cache做了修改，但是在第二个CPU的Cache中还是以前老的值，所以当线程1还没有拿到在线程2里面修改的b=1的值时，以及将其值赋值给x，x就为0了，同理y也有可能会被设置为0，所以除了重排序会导致序言中的DEMO变成(0,0)外，多线程的可见性也会导致这个值。





# 5. Memory Barriers

知道了可见性以及重排序是什么，那么我们就会问题如何解决重排序以及可见性的问题，在各个厂商的CPU都提供一个叫Memory Barriers(有的地方也叫Memory Fence)，中文就是内存屏障，他是用来告知CPU那些地方不需要重排序，那些地方需要立即可见，下面我就来说说这几种内存屏障。

#### load-load barriers(读读屏障)

禁止读2以及读2后面读指令和读1重排序；

#### store-store barriers(写写屏障)

1. 是确保写1的结果数据能立即被其他线程读到(会做一次Cache到Main Memory的写入操作，并使其他线程Cache对应共享数据过期);
2. 禁止写2以及写2后面的写指令和写1重排序；

#### load-store barriers(读写屏障)

1. 是确保写2的结果数据能立即被其他线程读到(会做一次Cache到Main Memory的写入操作，并使其他线程Cache对应共享数据过期);
2. 禁止写2以及写后的写指令和读1重排序；

#### store-load barriers(全能屏障)

1. 是确保写1之前的所有写指令的结果立即被其他线程读到(会做一次Full Cache到Main Memory的写操作，并使其他线程Cache对应共享数据过期)；
2. 禁止写1之后的所有指令重排序在写1以及写1之前的指令

细心一点儿可以看见和上面重排序的类型是对应的，Java内存模型在JIT编译时会根据不同的处理器架构来插入不同的屏障，来确保是按照程序员想要的顺序来的（前提是你加了同步关键字，下面章节会讲到，同步关键字和这些屏障的关系）；





# 6. volatile

### 6.1 使用

```java
int x = 0;
int y = 0;
volatile int a = 0;
volatile int b = 0;
```

### 6.2 语义

1. 执行写操作的时候，会将该写操作之前的所有写操作全部刷到Main Memory中，并告知接下来要读这次刷新被volatile变量线程过期，来使其他线程读取该变量做好准备；

2. 执行读操作的时候，判断是否过期，该读取的值过期会从Main Memory中加载最新的值到自己线程Cache区域中（而不会直接从Cache中读取，而且该读操作不光是当前变量，而且会把之前线程写操作之前写过的数据都拉一次，就算之前写的变量没有被volatile修饰），保证之前写操作对其他线程可见性的支持；

3. 禁止变量前后操作的重排序，禁止规则如下：

   | 能否重排序     | 第二个操作 | 第二个操作     | 第二个操作     |
   | --------- | ----- | --------- | --------- |
   | 第一个操作     | 普通读写  | volatile读 | volatile写 |
   | 普通读写      |       |           | NO        |
   | volatile读 | NO    | NO        | NO        |
   | volatile写 |       | NO        | NO        |

4. 来一个例子

   ```java
   thread 1:
   int x = 1;
   int y = 3;
   int z = 4;(z被volatile修饰)
   int j = 5;
   //假如上面代码都在一个线程中，x和y可以重排序，但是4和1/3/5之间都不能重排序，并且5也不能和1/3重排序；

   thread 2:
   int l = y;(new)
   int i = j;(old)
   int k = z(z被volatile修饰);
   //根据上面的语义，i的值是0而不是5，l的值时3而不是0(前提是z不会和y重排序，根据volatile重排序规则时有可能出现的)
   ```


###  6.3 如何实现

看到上面的语义，我们已经可以用volatile来修饰序言demo中x和y就可以避免(0,0)的出现，但是我们只知道了语义，但是volatile关键字是如何在Java内存模型让CPU规矩的按想要的语义执行了，这个就需要之前提到过的JIT动态编译的时候会插入内存屏障，下面是JMM插入屏障的策略：

- 在每个volatile写操作的前面插入一个store-store barriers;
- 在每个volatile写操作的后面插入一个store-load barriers;
- 在每个volatile读操作的后面插入一个load-load barriers;
- 在每个volatile读操作的后面插入一个load-store barries;

来一个例子：

```java
int a = 3;
volatile int b = 1;
volatile int c = 2;

void test() {
    int i = b;
    //插入load-load barriers, 禁止后面所有的读与读b重排序；
    //插入load-store barriers, 禁止后面所有的写于读b重排序（可以省掉，因为下面a的普通写已经被j洗面的内存屏障给挡掉了）；
    int j = c;
    //插入load-load barriers, 禁止后面所有的读与读b重排序（可以省掉，下面没有普通读了）；
    //插入load-store barriers, 禁止后面所有的写与读b重排序；
    a = i + j;
    //插入store-store barriers, 禁止后面所有的写与a重排序；
    b = 1;
    //插入store-load barriers, 禁止后面所指令与b重排序（可以省掉，因为下面跟了一个store-store屏障，没有必要使用全能型的store-load）；
    //插入store-store barriers, 禁止后面所有的写与a重排序； 
    c = 2;
    //插入store-load barriers, 禁止后面所指令与c重排序（需要将cache中修改过的数据全部刷到cache到其他线程知道，并且禁止后续所有未知指令的重排序）；
}
```

还没有完，如果这段代码运行在x86体系架构下面的处理插入的内存屏障会更少，屏障多虽然能保证一致性，但是也会使处理器的乱序执行的性能下降，所以JIT还会根据不同体系来减少内存屏障，我们在根据重排序那节讲到再来分析一下上面的例子：

```java
int a = 3;
volatile int b = 1;
volatile int c = 2;

void test() {
    int i = b;
    //插入load-load barriers, 禁止后面所有的读与读b重排序（可以省略，x86禁止load-load的重排序）；
    //插入load-store barriers, 禁止后面所有的写于读b重排序（可以省掉，因为下面a的普通写已经被j洗面的内存屏障给挡掉了）；
    int j = c;
    //插入load-load barriers, 禁止后面所有的读与读b重排序（可以省掉，下面没有普通读了）；
    //插入load-store barriers, 禁止后面所有的写与读b重排序（可以省略，x86禁止load-store的重排序）；
    a = i + j;
    //插入store-store barriers, 禁止后面所有的写与a重排序（可以省略，x86禁止store-store的重排序）；
    b = 1;
    //插入store-load barriers, 禁止后面所指令与b重排序（可以省掉，因为下面跟了一个store-store屏障，已经可以达到目的了，而且下面是volatile写，没有必要使用全能型的store-load）；
    //插入store-store barriers, 禁止后面所有的写与a重排序（可以省略，x86禁止store-store的重排序）； 
    c = 2;
    //插入store-load barriers, 禁止后面所指令与c重排序（需要将cache中修改过的数据全部刷到cache到其他线程知道，并且禁止后续所有未知指令的重排序）；
}
```

最后只剩下store-load类型的全能屏障，看到这里是不是觉得Java内存模型很强大，一个短小精悍的指令，帮你做了这么多事情，而且使适配了不同硬件平台，不得不说真牛逼；



### 6.4 不足

就这样volatile实现由于重排序和内存可见性导致的问题，但是他只能对单值的单一操作进行原子同步，到那时对于复杂意见需要多条语句的原子同步就显得不足了，比如 i++这种复合操作，其实是4个操作，用javap反编译class二进制文件可以得到如下结果：

```java
0:getstatic#13(获取i之前的值)
3:iconst_1(获取需要增加的1)
4:iadd(增加1)
5:putstatic#13(写回i变量)
//volatile只能保证getstatic#13的原子同步，而不能保证这4条指令的原子同步；对于这种复合操作，要么使用Atomic系列中的AtomicInteger，要么使用synchronized进行块或者方法同步；
```





# 7. synchronized

###  7.1 使用

```java
public synchronized void test() {}//这里其实是synchronized(this)

public void test() {
  synchronized(this) {//这里的this可以换成任何对象
  	//todo
  }
}
```

###  7.2 语义

其实synchronized使用的是监视锁，如下：

```java
public void test() {
  monitor.entry//lock
  //todo
  monitor.exit//unlock
}
```

monitor.entry是获取锁，如果其他线程进入同样的monitor.entry就需要等待，直到获取监视锁的那个线程执行到monitor.exit。

1. 执行monitor.enter时，会从Main Memory获取cache里面已经过期共享变量最新的值（和volatile读语义类似）；

2. 执行monitor.exit时，会将entry到exit中的所有写都刷新到Main Memory里面，并通知其他线程中的cache过期（和volatile写语义类似）；

3. monitor.enter和monitor.exit都会禁止重排序，但不会禁止enter到exit中的代码重排序，规则如下：

   | 能否重排序        | 第二个操作 | 第二个操作         | 第二个操作        |
   | ------------ | ----- | ------------- | ------------ |
   | 第一个操作        | 普通读写  | monitor.entry | monitor.exit |
   | 普通读写         |       |               | NO           |
   | monitor.enry | NO    | NO            | NO           |
   | monitor.exit |       | NO            | NO           |

   和volatile感觉是否类似，monitor.entry就是volatile读，monitor.entry就是volatile写；

### 7.3 如何实现

和volatile一样，JIT会插入一些屏障来避免重排序和让其他线程可见：

- 在每个monitor.exit操作的前面插入一个store-store barriers;
- 在每个monitor.exit操作的后面插入一个store-load barriers;
- 在每个monitor.entry操作的后面插入一个load-load barriers;
- 在每个monitor.entry操作的后面插入一个load-store barries;

再来看一个例子：

```java
int a;
volatile int v;
void test() {
  int i;
  synchronized(this) {
  //插入load-load barriers, 禁止a读取排序到entry之上，从而避免重排序导致逃离到sync同步块外面；
  //插入load-store barriers, 禁止a写入排序到entry之上；
    i = a;
    a = i;
  //store-store barriers, 禁止a写入排到exit之后(这个可以省略，entry下面有一个全能屏障)
  }
  //store-load barriers, 将enit之前的所有写操作都刷新到Main Memory，并且禁止后续指令重排序到前面；
  synchronized(this) {
    //插入load-load barriers, 禁止下面一个entry排序到该entry之上
    //插入load-store barriers(可以省略，因为下面是一个entry)
    synchronized(this) {
    //插入load-load barriers(可以省略，因为下面是一个exit)
    //插入load-store barriers(避免entry和下面的exit重排序)
    //插入store-store barriers(可以省略，上面没有store和exit)
    }
    //插入store-load barriers(避免上面exit和下面exit重排序，并且做一次full cache to main memory)
    //插入store-store barriers(可以省略,因为上面有一个全能store-load,因为要full cache所以这里只能用store-load)
  }
  //插入store-load barriers(避免下面读v与上面的exit重排序，并做一次full cache to memory)
  i = v;
  //插入load-load barriers, 禁止下面一个entry排序到该读v之上
  //插入load-store barriers(可以省略，因为下面是一个entry)
  synchronized(this) {
  //插入load-load barriers(可以省略，因为在exit之前没有读操作)
  //插入load-store barriers(避免entry与exit排序) 
  //插入store-store barriers(可以省略，因为在exit和上entry之间没有写操作) 
  }
  //插入store-load barriers(避免下面写v与上面的exit重排序，并做一次full cache to memory)
  //插入store-store barriers(可以省略，上面有store-load屏障) 
  v = i;
  //插入store-load barriers(避免下面entry与上面的写v重排序，并做一次刷cache to memory)
  synchronized(this) {
  //插入load-load barriers(可以省略，因为在exit之前没有读操作)
  //插入load-store barriers(避免entry与exit排序) 
  //插入store-store barriers(可以省略，因为在exit和上entry之间没有写操作)
  }
  //插入store-load barriers(并做一次刷cache to memory，避免后面位置指令排序到同步块中了)
}
```

同样JIT会根据不同处理器架构来去掉多余的屏障，这里就不再演示了。在补充一点就是在synchronized内部的普通变量之间重排序还是允许的，但是不能越过monitor的entry和exit两个边界。





# 8. final

### 8.1 作用

这里我们只讲对变量的修饰，被final修饰的变量只能在声明或者在构造函数内部进行赋值，其他地方只能读取使用，而且使用的时候已经之前初始化的值已经赋值好了；为什么在该篇文章要讲该关键字，是因为Java中的创建对象的过程不是原子性的，由于这个原因在JSR 133之前的内存模型出现过BUG，就是被final修饰的关键字，在多线程运行的情况下，读取到了没有在Java构造函数里面初始化好的值，这个已经违背了final关键字的定义，看下面的例子：

```java
public class FinalExample {                          
    final int i;                      
    static FinalExample obj;

    public void FinalExample () {     
        i = 1;                        
    }

    public static void writer () {    
        obj = new FinalExample ();
    }

    public static void reader () {       
        FinalExample object = obj;  
        if (object != null)
        	int a = object.i;                 
    }
}
```

线程1执行writer方法，线程2执行reader方法，由于在多线程下面并不知道他们之间数据依赖性；如果i是普通变量，很有可能i=1的赋值重排序obj = new FinalExample()之后，有可能导致线程2读取object.i的值是0，而不是1，所以在JSR 133里面新增加了final的内存语义：

### 8.2 语义：

禁止final变量的写排序到构造函数生成对象指针赋值给其他对象之后；

### 8.3 如何实现：

JMM会在构造函数结束前加一个store-store内存屏障，来确保构造函数里面的final修饰的变量不会重排序到构造函数生成对象指针赋值其他对象之后，避免了final变量的逸出；在x86体系下面这个store-store排序会被禁止的，所以JMM也不是在这个体系下面加这个屏障。





# 9. reentrantLock

### 9.1 作用

其作用和synchronized类似，都是对复杂的区域操作进行同步互斥操作，另外还多了一些高级特性，如：公平锁(synchronized是非公平的), 多条件线程通信机制, 可打断锁，定时锁，以及非结构加锁；在Java 1.5及1.5以前的版本synchronized性能是很糟糕的，但是在1.6以后得到了大幅度的提升，加入了偏量锁/自旋锁，最后才是调用操作系统的线程休眠方法，现在性能基本和reetrantlock持平了，优先使用synchronized加锁，需要高级特性的时候才使用显示锁。

### 9.2 实现

reetrantlock实现核心在volatile，利用了volatile重排序规则，以及可见性内存含义，我们还是来看代码，了解他是如何利用volatile变量的；

#### lock核心代码：

```java
protected final boolean tryAcquire(int acquires) {
    final Thread current = Thread.currentThread();
    int c = getState();   //获取锁的开始，首先读volatile变量state
    if (c == 0) {
        if (isFirst(current) &&
            compareAndSetState(0, acquires)) {
            setExclusiveOwnerThread(current);
            return true;
        }
    }
    else if (current == getExclusiveOwnerThread()) {
        int nextc = c + acquires;
        if (nextc < 0)  
            throw new Error("Maximum lock count exceeded");
        setState(nextc);
        return true;
    }
    return false;
}
```

注意compareAndSetState(0, acquires)，这个是CPU级别的原子操作CAS，比较是否等于0，如果是就把state设置新的acquires值，并返回true，如果不是等于0，直接返回false；CAS操作具有volatile的读和写操作，他有monitor.entry的内存语义，获取最新main memory内的最新值，并刷新自己thread的cache数据，而且也禁止lock之后的代码重排序到lock之前去；

#### unlock核心代码：

```java
protected final boolean tryRelease(int releases) {
    int c = getState() - releases;
    if (Thread.currentThread() != getExclusiveOwnerThread())
        throw new IllegalMonitorStateException();
    boolean free = false;
    if (c == 0) {
        free = true;
        setExclusiveOwnerThread(null);
    }
    setState(c);           //释放锁的最后，写volatile变量state
    return free;
}
```

注意setState(c)，如果c=0，将释放锁资源，并且做了一个state的volatile写操作，写操作的语义和monitor.exit语义类似，禁止exit后面指令排序到之前，并做了一次cache刷新到main memory的操作；

这里只是讲了和内存有关的reetrantlock的内容，由于篇幅，他是如何实现同步等待的这个感兴趣的童鞋可以自己行去review他整个代码。



# 10. happens-before

上面讲的那些内容是否很难记，也很难理解，其实 JSR-133作者（Goetz, Brian）也考虑为了方便大家理解提出了一个happens-before的规则，只要记住这些规则就知道那些同步关键字是如何工作的了：

#### 程序顺序规则

一个线程中的每个操作，happens- before 于该线程中的任意后续操作。

#### 监视器锁规则

对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。

#### volatile变量规则

对一个volatile域的写，happens- before 于任意后续对这个volatile域的读。

#### 传递性

如果A happens- before B，且B happens- before C，那么A happens- before C。

这些happens-before并不是说顺序上的，是只可见性上面，所以我个人觉得这个定义还是不完全准确，比如第一条JMM还是允许没有数据依赖之间指令排序，让其CPU在执行正确的基础上发挥最大的性能。第一条可以修改成这样：一个线程中每个操作happens-before该线程中依赖此操作的操作之前。

这些规则背后对应了若干上面提到的JMM如果应用内存屏障的规则，让开发人员尽量的容易理解JMM究竟怎么规范并发内存访问的；



# 11. 总结

说了这么多，JMM就行是什么？其实就是跨平台，跨硬件的帮你实现一致的并发访问内存数据解决方案，它能让CPU牺牲最小性能来保证并行方法问内存数据是按程序预期的。





# 12. 参考资源：

http://tech.meituan.com/java-memory-reordering.html

http://www.infoq.com/cn/articles/java-memory-model-1

http://www.cs.umd.edu/~pugh/java/memoryModel/jsr133.pdf

http://gee.cs.oswego.edu/dl/jmm/cookbook.html

https://www.cs.umd.edu/users/pugh/java/memoryModel/jsr-133-faq.html

《Java Concurrency in Practice》-Brian Goetz

《深入理解Java虚拟机》-周志明

