---
layout:     post
title:      "Spark Inside Park 1"
subtitle:   "Spark inside让普通开发人员可以通过该篇文档的学习可以快速上手Spark"
date:       2017-04-05 12:30:00
author:     "ryan"
header-img: "img/post-bg-02.jpg"
---

# 1. Spark背景介绍

Spark是一个用来实现快速而通用的集群计算平台。

## 1.1 出生背景

Spark于2009年，在著名的加州伯克利分校RAD实验室诞生，该实验室的人用过Hadoop Mapreduce，由于其效率低下，自己研发了Spark；

第一批用户是加州伯克利大学的其他研究组，然后慢慢有很多外部机构开始使用，社区活跃度随着Spark Meetups和Spark峰会，让Spark有了更多的使用者，以及源码贡献者(其中著名的贡献者有：雅虎，英特尔等)；

在2011年，开发了更多基于Spark高层组件，如Spark上的Hive，以及Spark Streaming等；

在2013年6月，RAD实验室把Spark交给了Apache基金会，现在已经成为Apache开源基金会的顶级项目；

## 1.2 Spark使用者

### 1.2.1 算法工程师

Spark支持Scale和Python的shell操作界面，并提供了大量的机器学习的算法实现(如：特征提取/统计/分类/回归/聚类/协同过滤/降维/以及模型评估函数等等)，让算法工程师可以通过shell，在大量算法支持上，使用Spark集群做快速的研究探索数据挖掘分析；

### 1.2.2 数据工程师

Spark SDK支持Scale/Java/Python，可以把算法工程师研究后的成果转成PRD可以运行的独立程序，并提供了Spark Streaming做实时计算，以及只要满足Hadoop数据接口的数据源，都可以在Spark进行读入和写出；现在基本大部分数据库都有类似的组件支持Spark的数据访问；



# 2. Spark Shell与独立应用

## 2.1 准备

首先你要下载Spark的软件包，我推荐大陆的用户直接到阿里云的镜像去下载：http://mirrors.aliyun.com/

下载后直接tar解压，无需安装，就可以使用，不过在使用之前，请确保已经安装了JDK7以上版本；

## 2.2 Shell

Python命令行启动：

```python
bin/pyspark
```

Scale命令行启动：

```python
bin/spark-shell
```

这里我们使用Scale来写一个例子，如果在控制在做数据分析；

```scala
scale> val lines = sc.textFile("README.md"); //创建一个名为lines的RDD
scale> lines.count();//统计RDD中的元素个数
scals> lines.first();//显示README.md的第一行元素
```

大家估计会问sc变量是什么，我都没有申明过。这个sc是在启动命令行的时候自动创建的，sc是类SparkContext的实例，该类是用来访问Spark集群的，sc代表了对Spark集群的一个连接；

## 2.3 独立应用

独立应用这里，我们就使用Java来做的，因为我们团队最熟悉该语言，按以下步骤：

1.开发独立应用之前，确保Eclipse，或者IntelliJ IDEA，并且安装了Maven;

2.创建一个maven项目，并添加Spark SDK的依赖包；

```xml
<dependency>
	<groupId>org.apache.spark</groupId>
	<artifactId>spark-core_${scale.version}</artifactId><!--可以修改成你下载的spark使用的scale版本-->
	<version>${spark.version}</version><!--可以修改成你下载的spark version-->
	<scope>provided</scope><!--注意这里是provided，因为spark每个节点的运行环境都有该核心引用包-->
</dependency>
```

3.编写如下代码：

```java
public class AppDemo {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("appDemo");
		JavaSparkContext sc = new JavaSparkContext(conf);
		JavaRDD<String> lines = sc.textFile("README.md");
		long count = lines.count();
		String firstline = lines.first();
		System.out.println(String.format("count:%d, firstline:%s", count, firstline));
	}
}
```

上面这段代码可shell是同样的效果，不过独立应用程序需要自己创建SparkContext；

5.mvn package该demo，然后用Spark-Submit将独立饮用程序提交的Spark中去执行：

```java
$SPARK_HOME/bin/spark-submit \
--class com.sunyuki.spark.test.AppDemo \ //入口函数类
target/app-demo.jar //程序包
```

以上，不管是shell，还是独立应用程序，都是运行在本地的，我们后面会单独讲，如果在Spark集群里面运行；



# 3. RDD操作

RDD是Spark分布式的元素集合，是一个对数据抽象的集合，并非真正我们传统意义上的集合，它是Spark的核心驱动。对数据的所有操作不外乎就是建立RDD，转化RDD，以及调用RDD进行求值，而这一切的背后，Spark会利用RDD讲数据分发到集群不同节点，来将操作并行化处理；

## 3.1 RDD创建

RDD创建支持两种途径，一种是读取外部数据集(如：读取外部的文件，数据库，流输入)；另一种一般用在测试，就是在Shell或者独立程序里面创建一个对象集合。

读取外部数据集：

```java
JavaRDD<String> lines = sc.textFile("README.md");
```

创建一个测试数据集：

```java
JavaRDD<String> lines = sc.parallelize(Arrays.asList("pandas","i likes pandas"));
```



## 3.2 RDD转化操作

RDD操作分为转换和行动操作的，如何区分是行动和转化，最简单的办法就是看它操作返回的是RDD，还是具体的值，以下是常见的转化操作：

| 函数名(操作名)       | 目的                                       | 示例:{1,2,3,3}              | 结果                    |
| -------------- | ---------------------------------------- | ------------------------- | --------------------- |
| map()          | 将RDD抽象集合中的每个元素，一个一个的加工生成新的RDD，是一个一对一的转化，即转化后不会增减元素数据，只是加工每个元素； | rdd.map(x => x+1)         | {2,3,4,4}             |
| flatMap()      | 将RDD抽象集合中的每个元素，从一个变成多个，转化后对数量会增加，该函数一般用在分割每行文本成单个值； | rdd.flatMap(x => x.to(3)) | {1,2,3,2,3,3,3}       |
| filter()       | 将RDD抽象集合转化为一个满足过滤条件的新RDD抽象集合；            | rdd.filter(x => x != 1)   | {2,3,3}               |
| distinct()     | 去重操作                                     | rdd.distinct()            | {1,2,3}               |
| union()        | 和sql中的union一样，将两个抽象的RDD集合合并起来，不去重；示例数据：rdd1={1,2,3}, rdd2={3,4,5} | rdd1.union(rdd2)          | {1,2,3,3,4,5}         |
| intersection() | 取两个集合的共同交集，示例数据如union                    | rdd1.intersection(rdd2)   | {3}                   |
| substract()    | 移除一个集合中在另外一个集合中公有的内容，示例数据如union，(这个在协同过滤推荐会用的比较多) | rdd1.substract(rdd2)      | {1,2}                 |
| cartesian()    | 计算两个抽象集合的交叉笛卡尔组合，返回是一个键值对的抽象集合           | rdd1.cartesian(rdd2)      | {(1,3),{1,4},..(3,5)} |

在Java环境中的代码例子：

```java
public class AppTransfer {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("appTransfer");
		JavaSparkContext sc = new JavaSparkContext(conf);
		JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1,2,3,3));
		
		//map example:
		JavaRDD<Integer> rdd2 = rdd1.map(new Function<Integer,Integer>() {
			public Integer call(Integer arg0) throws Exception {
				return arg0 + 1;
			}
		});
		
		//flapMap example:
		JavaRDD<Integer> rdd3 = rdd1.flatMap(new FlatMapFunction<Integer,Integer>() {
			public Iterable<Integer> call(Integer arg0) throws Exception {
				ArrayList<Integer> vals = new ArrayList<Integer>();
				for(int i = 0; i < arg0; i++) {
					vals.add(i);
				}
				return vals;
			}
		});
	}
}
```



## 3.2 RDD行动操作

行动操作是需要具体产生某个结果然后进行输出的，常见到的行动操作有：

| 函数名(操作名)                   | 目的                                       | 示例{1,2,3,3}               | 结果                  |
| -------------------------- | ---------------------------------------- | ------------------------- | ------------------- |
| collect()                  | 返回抽象集合中的所有元素，要么是Map，要么是List，(一般在PRD用的很少，因为会把所有数据加载到本地的一个分区内存中，会导致，单台机器内存崩掉） | rdd.collect()             | {1,2,3,3}           |
| count()                    | 返回抽象集合中的元素个数，类型long                      | rdd.count()               | 4                   |
| countByValue()             | 返回抽象集合中的各个元素出现的次数，类型Map                  | rdd.countByValue()        | {(1,1),(2,1),(3,2)} |
| take(num)                  | 从RDD按自然顺序取最早的num个元素                      | rdd.take(2)               | {1,2}               |
| top(num)                   | 从RDD按自然顺序取最近的num个元素                      | rdd.top(2)                | {3,3}               |
| takeOrdered(num)(ordering) | 按照指定的排序规则进行去最前面的num元素                    | rdd.take(2)(myOrdering)   | {3,3}               |
| reduce(func)               | 将一个RDD抽象数据集，聚合成一个值的操作，比如sum              | rdd.reduce((x,y) => x+y); | 9                   |
| foreach(func)              | 迭代一个RDD抽象数据集                             | rdd.foreach(func)         | 一行一行实际元素数据          |

在Java环境中的代码例子：

```java
public class AppExecute {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("appExecute");
		JavaSparkContext sc = new JavaSparkContext(conf);
		JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("Hadoop","Spark","MongoDB","Hdfs"));
		
		//foreach example:
		rdd1.foreach(new VoidFunction<String>() {
			public void call(String arg0) throws Exception {
				System.out.println(arg0);
			}
		});
		
	}
}
```

在Java里面要实现方法作为参数进行传递，还是需要借助类来实现，上面都使用到了继承接口实现方案的局部匿名内部类来实现的，下图是Spark定义的这些Function来支持不同转化和行动类型的RDD；

![2017-04-13-spark-function](/Users/ryan/Documents/git/ryanwli.github.io/img/2017/2017-04-13-spark-function.png)



## 3.3 RDD惰性求值

前面提到的创建RDD，以及转化类型的RDD，并不是像传统的Java程序一样执行到那一步就会进行执行，而这些转化类型的RDD会等到第一个行动类型的RDD操作出现，才会真正开始执行，所以说转化类型的RDD是惰性的，需要行动类型RDD来驱动；



## 3.4 RDD持久化

讲持久化之前，我们先来看一个例子：

```java
public class AppDemo {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("appDemo");
		JavaSparkContext sc = new JavaSparkContext(conf);
		
		//转化类型RDD
		JavaRDD<String> lines = sc.textFile("README.md");
		JavaRDD<String> newLines = lines.map(new Function<String,String>() {
			public String call(String v1) throws Exception {
				return "appDemo: " + v1;
			}
		});
		
		//行动类型RDD
		long count = newLines.count();
		String firstline = newLines.first();
		System.out.println(String.format("count:%d, firstline:%s", count, firstline));
	}
}
```

之前我说过返回值是RDD类型的就是转化操作，然后返回具体值的就是行为操作。其中count()和first()都是行为操作，执行到count()或者first()开会开始读取README.md文件，以及执行map转换，不过这里两个求值操作，所以会读取和map各两次。为了避免这种无谓的重复开销，我们可以使用RDD的persist函数，讲数据缓存起来，下面这些行动操作就直接使用缓存，就不会每次都去读取文件以及map了；persist接受一个存储方法，如下表：

| 级别                  | 使用的空间 | CPU时间 | 是否在内存中 | 是否在磁盘上 | 备注                                |
| ------------------- | :---: | :---: | :----: | :----: | :-------------------------------- |
| MEMORY_ONLY         |   高   |   低   |   是    |   否    |                                   |
| MEMORY_ONLY_SER     |   低   |   高   |   是    |   否    |                                   |
| MEMORY_AND_DISK     |   高   |  中等   |   部分   |   部分   | 如果数据在内存中放不下，则溢写到磁盘上               |
| MEMORY_AND_DISK_SER |   低   |   高   |   部分   |   部分   | 如果数据在内存中放不下，则溢写到磁盘上。在内存中存放序列化后的数据 |
| DISK_ONLY           |   低   |   高   |   否    |   是    |                                   |



## 3.5 Pair RDD操作

上面我们基本都讲的List类型的RDD操作，Spark提供键值对的RDD操作，在Spark中叫Pair RDD；

常用的转化类型的RDD函数：

| 函数名                 | 目的                                       | 示例{(1,2),(3,4),(3,6)}                    | 结果                                       |
| ------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- |
| reduceByKey(func)   | 合并具有相同键的值                                | rdd.reduceByKey((x,y)=>x+y)              | {(1,2),(3,10)}                           |
| groupByKey()        | 对具有相同键的值分组                               | rdd.groupByKey()                         | {(1,[2]),(3,[4,6])}                      |
| mapValues(func)     | 对每个pair RDD中的每个值进行修改，而不改变键               | rdd.mapValues(x=>x+1)                    | {(1,3),(3,5),(3,7)}                      |
| flatMapValues(func) | 对pair中的每个值进行迭代生成多个值的操作。                  | rdd.flatMapValues(x => (x to 5))         | {(1,2),(1,3),(1,4),(1,5),(3,4),(3,5)}    |
| keys()              | 返回仅仅包含只有Key的RDD抽象数据集                     | rdd.keys()                               | {1,3,3}                                  |
| values()            | 返回仅仅包含只有Value的RDD抽象数据集                   | rdd.values()                             | {2,4,6}                                  |
| sortByKey()         | 返回一个根据键排序的RDD                            | rdd.sortByKey()                          | {(1,2),(3,4),(3,6)}                      |
| mapToPair()         | 将一个List的RDD抽象集合转成Pair类型的RDD,示例rdd={"ryan ni","aaron yan","jerry mi"} | rdd.mapToPair(x=> (x.split(" ")[0],x.split(" ")[1])); | {("ryan","ni"),("aaron","yan"),("jerry","mi")} |

转化类型RDD的Java的例子：

```java
public class AppPairTransfer {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("appPairTransfer");
		JavaSparkContext sc = new JavaSparkContext(conf);
		JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1,2,3,3));
		
		//mapToPair example:
		JavaPairRDD<Integer,Integer> rdd2 = rdd1.mapToPair(new PairFunction<Integer,Integer,Integer>() {
			public Tuple2<Integer, Integer> call(Integer t) throws Exception {
				return new Tuple2(t,1);
			}
		});
		
		//reduceByKey example:
		JavaPairRDD<Integer,Integer> rdd3 = rdd2.reduceByKey(new Function2<Integer,Integer,Integer>() {
			public Integer call(Integer v1, Integer v2) throws Exception {
				return v1 + v2;
			}
		});
	}
}
```

常用的行动类型的RDD函数：

| 函数名          | 描述                        | 示例{(1,2),(3,4),(3,6)} | 结果                     |
| ------------ | ------------------------- | --------------------- | ---------------------- |
| countByKey   | 对每个键对应的元素分别计数结果集          | rdd.countByKey()      | {(1,2),(3,2)}          |
| collectAsMap | 同List的RDD的collect函数类似(慎用) | rdd.collectAsMap()    | Map[(1,2),(3,4),(3,6)] |
| lookup(key)  | 返回给定键对应的所有值               | rdd.lookup(3)         | [4,6]                  |

行动类型的RDD的Java例子：

```java
public class AppPairTransfer {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("appPairTransfer");
		JavaSparkContext sc = new JavaSparkContext(conf);
		List<Tuple2<Integer, Integer>> list = new ArrayList<Tuple2<Integer, Integer>>();
		list.add(new Tuple2(1,2));
		list.add(new Tuple2(3,4));
		list.add(new Tuple2(3,6));
		JavaPairRDD<Integer, Integer> rdd1 = sc.parallelizePairs(list);
		
		//countByKey example:
		Map<Integer, Object> result1 = rdd1.countByKey();
		//collectAsMap
		Map<Integer, Integer> result2 = rdd1.collectAsMap();
		//lookup
		List<Integer> result3 = rdd1.lookup(3);
	}
}
```



## 3.6 RDD创建分区

RDD分区数代表了该RDD抽象数据集被分成多少真实数据集进行并行计算。原则上该值尽可能等于集群核心数；下面我们来说一下常见几种创建RDD时分区大小，在spark conf里面设置spark.default.parallelism，可以制定默认的并行分区数量，如果不指定会根据不同运行环境来动态计算（本地运行，独立集群，Yarn集群等）；

parallelize方式创建：

```java
//只会在本地，根据本地机器CPU核数来指定分区数量
JavaPairRDD<Integer, Integer> rdd1 = sc.parallelizePairs(list);
//显示制定分区数量为2
JavaPairRDD<Integer, Integer> rdd1 = sc.parallelizePairs(list,2);
```

textFile方式创建：

```java
//不指定分区数，会取default分区数和2其中最小的；读取本地文件运行的不是在本地机器，而是集群，如果其他集群没有该文件的拷贝，那么将在集群中执行失败；
JavaRDD<String> lines = sc.textFile("README.md");
```

textFile Hdfs方式创建：

```java
//分区数会按照hdfs文件的分块数来决定，但是真正并行执行的分区数，还是不会超过spark.default.parallelism，下面有一图来说明Hdfs下面如何将数据放到不同分区的；
JavaRDD<String> lines = sc.textFile("hdfs://remoteip:port/floder/README.md");
```

![2017-04-13-spark-hdfs-partition](/Users/ryan/Documents/git/ryanwli.github.io/img/2017/2017-04-13-spark-hdfs-partition.png)

Hdfs是将一个文件分层若干个block，一个block默认大小64MB(可以配置)，如果README.md有190MB，那么将会被分成3个Block，然后Spark会根据Hdfs的Block生成3个SplitInput数据(如果Block配置的较小，SplitInput会对应多个Block, 但是对应的多个block都是属于一个文件的)，然后会生成读取数据任务的Task，Task会被分配到集群中的不同节点，不同节点会有一个或者多个Executor，Executor又会开多个线程，最后执行这些Task的就是这些节点上Executor中的一个线程来处理，最后Task执行完后会生成一个对应的分区处理集合。所以从这个图可以看出，分区数已经由Block的节点数决定了；

mongoDB方式创建：

```java
JavaPairRDD<Object,BSONObject> readfile = sc.newAPIHadoopRDD(mongoConfig, MongoInputFormat.class, Object.class, BSONObject.class);
```

mongoDB读取方式类似hdfs上面那图，唯一区别是在SplitInput这部的时候，mongoDB会获得整个collection的大小，然后更具_id进行split成N批，然后后面的步骤都是一样的了。

这里一直在说分区，不管是在本地，还是在集群，都会有分区产生；因为本地也可以使用线程执行Task。关于集群的配置，我们在后续文章给出；

## 3.7 Pair RDD重分区

由于我们从外部数据元读入的数据可能是混乱的，在读入之后，我们需要进行重分区，比如将PairRDD的Hash Key值相等的放入一个分区，我们就可以使用。

```java
JavaPairRDD<Integer, Integer> rdd1 = sc.parallelizePairs(list,2);
JavaPairRDD<Integer, Integer> rdd2 = rdd1.partitionBy(new HashPartitioner(10));
```

这样在做Join, GroupByKey, ReduceByKey等等这些操作都能在重分区中获益，因为特别是在数据量特别大的时候，重新分区后极大减少了在后续更多分布式计算时候的，数据在不同节点同步的网络开销；



# 4. 本章结语

本章说了Spark的一些基础知识，并将一下一点儿稍微进阶的分区知识；后面的一篇，我们会说说如何配置Spark分布式集群，如何实现Kafka与Spark Streaming集成，如何和mongoDB集成，以及hdfs集成；